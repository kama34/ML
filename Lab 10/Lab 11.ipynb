{"cells":[{"cell_type":"markdown","metadata":{"id":"PZKrlwajuGZ-"},"source":["# Week 11: Ensemble Learning\n","\n","```\n","- Machine Learning, Innopolis University (Fall semester 2023)\n","- Professor: Adil Khan\n","- Teaching Assistant: Gcinizwe Dlamini\n","```\n","<hr>\n","\n","\n","```\n","In this lab, you will practice Ensemble learning\n","\n","Lab Plan :\n","    1. Ensemble learning\n","    2. Bagging\n","    3. Random Forests\n","    4. AdaBoost\n","```\n","\n","<hr>\n","\n","Why ensemble learning? How does it help? <span style=\"color:blue\"> By combining the power of multiple models in a single model while overcoming their weaknesses, thus reducing variance and/or bias."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"O4UZ7nkDo0_2","executionInfo":{"status":"ok","timestamp":1701108983891,"user_tz":-180,"elapsed":5,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"d97fd8c8-fd7d-4cce-895e-0afb36724603"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>.container { width:100% !important; }</style>"]},"metadata":{}}],"source":["from IPython.display import display, HTML\n","display(HTML(\"<style>.container { width:100% !important; }</style>\"))"]},{"cell_type":"markdown","metadata":{"id":"4G2NXb2yznzd"},"source":["## Ensemble learning\n","We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n","\n","Let's start from uploading MNIST dataset."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"id":"9sI82NDtzoSP","outputId":"5eb850b5-a7c2-4cee-8785-de8e4d1a9c8e","executionInfo":{"status":"ok","timestamp":1701108985667,"user_tz":-180,"elapsed":1779,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 300x300 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","digits = load_digits()\n","X = digits.data\n","y = digits.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n","\n","\n","plt.figure(1, figsize=(3, 3))\n","plt.imshow(X[1].reshape((8,8)), cmap=\"gray\")\n","plt.xticks([])\n","plt.yticks([])\n","plt.title(f\"label is {y[1]}\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TcJSouftKwUC"},"source":["### Single decision tree\n","\n","First, we train a single decision tree."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqrQbHVDKw9F","outputId":"92d0b19e-12c7-418c-8696-7f47e87d76bc","executionInfo":{"status":"ok","timestamp":1701108985667,"user_tz":-180,"elapsed":12,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Single tree accuracy: 0.877104377104377\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","tree = DecisionTreeClassifier()\n","tree.fit(X_train, y_train)\n","pred = tree.predict(X_test)\n","tree_score = accuracy_score(y_test, pred)\n","print(\"Single tree accuracy:\", tree_score)"]},{"cell_type":"markdown","metadata":{"id":"qgAj4l5GLKuG"},"source":["Note the accuracy - it is around **0.85**.\n","\n","## 1. Bagging\n","\n","\n","What is decreased by bagging? Variance or bias? How? <span style=\"color:blue\"> Averaging reduces variance.\n","$$Var\\left(\\frac{1}{n}\\sum_{n=1}^{n}x_i\\right) = \\frac{1}{n^2}var\\left(\\sum_{n=1}^{n}x_i\\right) = \\frac{1}{n^2}\\left(\\sum_{n=1}^{n}var\\left(x_i\\right)\\right) = \\frac{1}{n^2} \\sum_{n=1}^{n} \\sigma^2 = \\frac{1}{n^2} * n * \\sigma^2 = \\frac{\\sigma^2}{n}$$\n","    \n","\n","\n","Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmjIUjxUo0_4","executionInfo":{"status":"ok","timestamp":1701108987236,"user_tz":-180,"elapsed":1577,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}},"outputId":"5b0e5f6c-a32e-49dc-8752-c0d6e97ed00e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 2 2 ... 2 2 2]\n"," [2 8 8 ... 8 9 8]\n"," [2 2 2 ... 2 2 2]\n"," ...\n"," [1 1 0 ... 1 8 1]\n"," [1 1 2 ... 2 1 8]\n"," [2 2 2 ... 2 2 2]]\n","Bagging accuracy: 0.9478114478114478\n"]}],"source":["import numpy as np\n","from scipy.stats import mode\n","\n","n_trees = 100\n","\n","classifiers = []\n","for i in range(n_trees):\n","    # train a new classifier and append it to the list\n","    indx = np.random.randint(0,len(y_train),len(y_train))\n","    tree = DecisionTreeClassifier()\n","    tree.fit(X_train[indx], y_train[indx])\n","    classifiers.append(tree)\n","\n","# here we will store predictions for all samples and all base classifiers\n","base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n","for i in range(n_trees):\n","    # obtain the predictions from each tree\n","    base_pred[:,i] = classifiers[i].predict(X_test)\n","\n","print(base_pred)\n","\n","# aggregate predictions by majority voting\n","pred = mode(base_pred, axis=1)[0].ravel()\n","acc = accuracy_score(y_test, pred)\n","print(\"Bagging accuracy:\", acc)"]},{"cell_type":"markdown","source":["Note the accuracy - it is around 0.95."],"metadata":{"id":"7zsxpa0HtZUp"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWQTCYrGLLMM","outputId":"5fd51d05-0ffb-4c5a-fb13-37bfc48f2296","executionInfo":{"status":"ok","timestamp":1701108989948,"user_tz":-180,"elapsed":2717,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[8 3 3 ... 8 3 3]\n"," [8 8 8 ... 8 8 8]\n"," [2 2 2 ... 2 2 2]\n"," ...\n"," [1 1 1 ... 1 1 1]\n"," [1 1 1 ... 1 1 1]\n"," [2 2 2 ... 2 2 2]]\n","Bagging accuracy: 0.8855218855218855\n"]}],"source":["import numpy as np\n","from scipy.stats import mode\n","\n","n_trees = 100\n","\n","classifiers = []\n","for i in range(n_trees):\n","    # train a new classifier and append it to the list\n","    tree = DecisionTreeClassifier()\n","    tree.fit(X_train, y_train)\n","    classifiers.append(tree)\n","\n","# here we will store predictions for all samples and all base classifiers\n","base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n","for i in range(n_trees):\n","    # obtain the predictions from each tree\n","    base_pred[:,i] = classifiers[i].predict(X_test)\n","\n","print(base_pred)\n","\n","# aggregate predictions by majority voting\n","pred = mode(base_pred, axis=1)[0].ravel()\n","acc = accuracy_score(y_test, pred)\n","print(\"Bagging accuracy:\", acc)"]},{"cell_type":"markdown","source":["Note the accuracy - it is around 0.88."],"metadata":{"id":"L2wECucTu39n"}},{"cell_type":"markdown","metadata":{"id":"ODOPvJJ2bKPh"},"source":["Now the accuracy grew up to **0.88**. You can see that our classifiers return very similar results. By the way, why the base classifiers are not identical at all? <span style=\"color:blue\"> That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hkrJAae5o0_4"},"source":["## 2. Random forest\n","\n","Compared to simple bagging we've just implemented, random forest can show better results because base classifiers are much less correlated.\n","\n","<span style=\"color:red\">TODO: At first, implement bootstrap sampling (use numpy)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9d1ElFpE48c","outputId":"1e57ed14-17ec-4ce2-8c74-1b10860cdaa2","executionInfo":{"status":"ok","timestamp":1701108989949,"user_tz":-180,"elapsed":7,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 0,  1,  2],\n","        [ 9, 10, 11],\n","        [ 3,  4,  5],\n","        [ 0,  1,  2]]),\n"," array([0, 3, 1, 0]))"]},"metadata":{},"execution_count":6}],"source":["def bootstrap(X, y):\n","    indices = np.random.randint(0, len(y), len(y))\n","    return X[indices], y[indices]\n","\n","np.random.seed(0)\n","a = np.array(range(12)).reshape(4,3)\n","b = np.array(range(4))\n","bootstrap(a, b)"]},{"cell_type":"markdown","metadata":{"id":"NxpCck94Y73A"},"source":["You should get\n","\n","(array([[ 0,  1,  2], <br>\n","&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n","&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n","&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n","array([0, 3, 1, 0]))\n","       \n","Now let's build a set of decision trees, each of them is trained on a bootstrap sampling from X and $\\sqrt d$ features."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxIhmI_H5jnI","outputId":"30755668-8ebf-4af7-fdd3-ea589b1901cf","executionInfo":{"status":"ok","timestamp":1701108990744,"user_tz":-180,"elapsed":801,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Random forest accuracy: 0.9814814814814815\n"]}],"source":["classifiers = []\n","for i in range(100):\n","    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n","    base = DecisionTreeClassifier(max_features=\"sqrt\")\n","    bs_X, bs_y = bootstrap(X_train, y_train)\n","    base.fit(bs_X, bs_y)\n","    classifiers.append(base)\n","\n","base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n","for i in range(n_trees):\n","    base_pred[i,:] = classifiers[i].predict(X_test)\n","\n","pred = mode(base_pred, axis=0)[0].ravel()\n","acc = accuracy_score(y_test, pred)\n","print(\"Random forest accuracy:\", acc)"]},{"cell_type":"markdown","source":["Note the accuracy - it is around 0.98."],"metadata":{"id":"RZd-_CNZx1C7"}},{"cell_type":"markdown","metadata":{"id":"ObGBbQfrE5jM"},"source":["And now we got **0.97** accuracy, which is a significant improvement! Now you can see why it is so important to have diverse classifiers."]},{"cell_type":"markdown","metadata":{"id":"qp4WWdYezpHy"},"source":["## 3. Boosting\n","\n","How does boosting work? <span style=\"color:blue\"> Models are built sequentially: each model is built using information from previously built models. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n","\n","For simplicity let's make a binary classification problem out of the original problem."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"JBbO7p-aNk69","executionInfo":{"status":"ok","timestamp":1701108990744,"user_tz":-180,"elapsed":2,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[],"source":["y_train_b = (y_train == 2 ) * 2 - 1\n","y_test_b = (y_test == 2 ) * 2 - 1"]},{"cell_type":"markdown","metadata":{"id":"talvfivTQitE"},"source":["Now let's train a boosting model.\n","\n","We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n","\n","Tree weight $w$ is computed using weighted error or $1 - accuracy$\n","\n","$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n","\n","For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights.\n","\n","\n","## 3.1 Boosting from Scratch\n","\n","Tasks:\n","1. initialize sample weights\n","1. caclulate tree weight\n","1. update sample weights\n","1. normalize the weights"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"mVEBRi4pEwte","executionInfo":{"status":"ok","timestamp":1701108991180,"user_tz":-180,"elapsed":438,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[],"source":["n_trees = 10\n","tree_weights = np.zeros(n_trees)\n","classifiers = []\n","train_samples = X_train.shape[0]\n","\n","# TODO: initialize sample weights\n","sample_weights = np.ones(train_samples) / train_samples\n","\n","learning_rate = 0.1\n","\n","for i in range(n_trees):\n","    clf = DecisionTreeClassifier(min_samples_leaf=3)\n","    clf.fit(X_train, y_train_b, sample_weight=sample_weights)\n","    pred = clf.predict(X_train)\n","    acc = accuracy_score(y_train_b, pred, sample_weight=sample_weights)\n","\n","    # TODO: caclulate tree weight\n","    epsilon = 1e-10\n","    w = 0.5 * np.log((1 - acc + epsilon) / (acc + epsilon))\n","    tree_weights[i] = w\n","    classifiers.append(clf)\n","\n","    # TODO: update sample weights\n","    gradient = -2 * (y_train_b - pred) * pred * (1 - pred)\n","    sample_weights *= np.exp(-learning_rate * gradient)\n","\n","    # TODO: normalize the weights\n","    sample_weights /= np.sum(sample_weights)"]},{"cell_type":"markdown","metadata":{"id":"YJ43LIorSXbs"},"source":["Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n","\n","$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpJKjCDdzpmt","outputId":"c5e7e44c-d965-4b86-a23d-b0eb0a2419b3","executionInfo":{"status":"ok","timestamp":1701108991181,"user_tz":-180,"elapsed":7,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Boosting accuracy: 0.9797979797979798\n"]}],"source":["n_test = X_test.shape[0]\n","\n","pred = np.zeros(n_test)\n","# caclulate predictions\n","for t in range(n_trees):\n","    pred += classifiers[t].predict(X_test) * tree_weights[t]\n","for i in range(n_test):\n","    pred[i] = -1 if pred[i] > 0 else 1\n","\n","\n","acc = accuracy_score(y_test_b, pred)\n","print(\"Boosting accuracy:\", acc)"]},{"cell_type":"markdown","metadata":{"id":"E7mv1TfwSahW"},"source":["The resulting accuracy is **0.97**"]},{"cell_type":"markdown","metadata":{"id":"a_khS4roo0_5"},"source":["## Self practice task\n","\n","Using dataset from assignment 2 (Task 1)\n","* Train and evaluate the following models using default parameters:\n","    1. Decision Tree\n","    1. Random Forest\n","    1. Adaptive boosting model\n","    1. [Catboost from Yandex](https://catboost.ai/en/docs/concepts/python-quickstart)\n","    1. [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/)\n","* Apply hyperparameters tuning for the models listed above and compare the models performance and also training time.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oKdYRtj4o0_6","executionInfo":{"status":"ok","timestamp":1701108991181,"user_tz":-180,"elapsed":5,"user":{"displayName":"Дмитрий Камышников","userId":"03871553027115189109"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}